{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "### Practical Session\n",
    "\n",
    "<br/> Prof. Dr. Georgios K. Ouzounis\n",
    "<br/> email: georgios.ouzounis@go.kauko.lt\n",
    "<br/>Changed to MasterCard by Rokas Kilƒçiauskas KT-9/1\n",
    "<br/>email: rokas.ki9765@go.kauko.lt\n",
    "<br/>I used MasterCard stocks From 2009 January 1st to 2019 December 31st (10 years) for better training to get better predictions. \n",
    "<br/>Number of time step: 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. Data loading and pre-processing\n",
    "2. Building the RNN\n",
    "3. Train and deploy the RNN\n",
    "4. Improving the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a 5-year history of the Google Stock prices predict the stock values for the period of the recent most month that are not included in the historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data-sets\n",
    "\n",
    "The data-sets are two comma-separated values files (CSV) and contain a data table of 1258 records for training and a table of 21 records for testing.\n",
    "\n",
    "They can be found at the [Kaggle.com website](https://www.kaggle.com/akram24/google-stock-price-train) or at various web locations after searching for the filenames: \n",
    "\n",
    "**Google_Stock_Price_Test.csv** and **Google_Stock_Price_Train.csv**\n",
    "\n",
    "Known alternative location: Github user [pdway53](https://github.com/pdway53/Predict_Google_Stock_Price_RNN) \n",
    "\n",
    "Open a terminal and use the wget command to get it of the selected location. Example:\n",
    "\n",
    "```shell\n",
    "wget https://raw.githubusercontent.com/pdway53/Predict_Google_Stock_Price_RNN/master/Google_Stock_Price_Test.csv\n",
    "\n",
    "wget https://raw.githubusercontent.com/pdway53/Predict_Google_Stock_Price_RNN/master/Google_Stock_Price_Train.csv \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "We need 3 main libraries:\n",
    "\n",
    "- [Numpy](http://www.numpy.org): it is the fundamental package for scientific computing with Python. It contains among other things a powerful N-dimensional array object that can be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined.\n",
    "- [matplotlib](https://matplotlib.org):  it is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\n",
    "- [pandas](https://pandas.pydata.org): is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset description: the open high, low and close values of the MasterCard Stock from 2009 to 2019. [Relevant code here](https://github.com/RokasKilc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "\n",
    "# load the file contents \n",
    "dataset_train = pd.read_csv('Mastercard_Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>14.410</td>\n",
       "      <td>15.070000</td>\n",
       "      <td>14.273</td>\n",
       "      <td>14.978</td>\n",
       "      <td>14.029945</td>\n",
       "      <td>22311000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-05</td>\n",
       "      <td>14.955</td>\n",
       "      <td>15.684000</td>\n",
       "      <td>14.903</td>\n",
       "      <td>15.326</td>\n",
       "      <td>14.355920</td>\n",
       "      <td>24719000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>15.626</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>15.529</td>\n",
       "      <td>16.202</td>\n",
       "      <td>15.176473</td>\n",
       "      <td>43357000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-07</td>\n",
       "      <td>15.810</td>\n",
       "      <td>16.061001</td>\n",
       "      <td>14.911</td>\n",
       "      <td>15.207</td>\n",
       "      <td>14.257652</td>\n",
       "      <td>39537000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-08</td>\n",
       "      <td>15.096</td>\n",
       "      <td>15.296000</td>\n",
       "      <td>14.836</td>\n",
       "      <td>15.278</td>\n",
       "      <td>14.324217</td>\n",
       "      <td>22841000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date    Open       High     Low   Close  Adj Close    Volume\n",
       "0  2009-01-02  14.410  15.070000  14.273  14.978  14.029945  22311000\n",
       "1  2009-01-05  14.955  15.684000  14.903  15.326  14.355920  24719000\n",
       "2  2009-01-06  15.626  16.500000  15.529  16.202  15.176473  43357000\n",
       "3  2009-01-07  15.810  16.061001  14.911  15.207  14.257652  39537000\n",
       "4  2009-01-08  15.096  15.296000  14.836  15.278  14.324217  22841000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subtable of relevant entries (open values)\n",
    "# The .values makes this vector a numpy array\n",
    "training_set = dataset_train.iloc[:, 1:2].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 14.41    ],\n",
       "       [ 14.955   ],\n",
       "       [ 15.626   ],\n",
       "       ...,\n",
       "       [280.600006],\n",
       "       [283.119995],\n",
       "       [279.339996]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy arrays do not support the view() and head() methods. [More on accessing the numpy data](https://jakevdp.github.io/PythonDataScienceHandbook/02.02-the-basics-of-numpy-arrays.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Next we need to rescale our data to the range from 0 to 1. \n",
    "\n",
    "Feature scaling is essential as discussed if the Features lecture and needs to be applied to both the training and test sets.\n",
    "\n",
    "It is computed using the ScikitLearn library [MinMaxScaler()](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler) which transforms the selected feature by scaling it to a given range. If more than one, this estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "# import the MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scaler instance to rescale all data to the range of 0.0 to 1.0 \n",
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the actual training set of scaled values\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00852336],\n",
       "       [0.01053428],\n",
       "       [0.01301011],\n",
       "       ...,\n",
       "       [0.99070183],\n",
       "       [1.        ],\n",
       "       [0.98605269]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the training set to dependent and independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1bckuLGZCeLUzNA-xJCGOODzC-4n2U-If\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with 90 timesteps and 1 output\n",
    "\n",
    "# the 90 stock prices in the last 3 months before today\n",
    "X_train = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2661, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find traning range (rows in .cvs file)\n",
    "training_set_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the stock price today\n",
    "y_train = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go back 90 days\n",
    "for i in range(90, 2661): \n",
    "    # 0 is the column ID, the only column in this case.    \n",
    "    # put the last 90 days values in one row of X_train\n",
    "    X_train.append(training_set_scaled[i-90:i, 0]) \n",
    "    y_train.append(training_set_scaled[i, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00852336, 0.01053428, 0.01301011, ..., 0.02280275, 0.02235997,\n",
       "        0.02268837],\n",
       "       [0.01053428, 0.01301011, 0.01368903, ..., 0.02235997, 0.02268837,\n",
       "        0.01955575],\n",
       "       [0.01301011, 0.01368903, 0.01105453, ..., 0.02268837, 0.01955575,\n",
       "        0.01807985],\n",
       "       ...,\n",
       "       [0.80968193, 0.80300347, 0.79735813, ..., 0.98848795, 0.98173573,\n",
       "        0.98848795],\n",
       "       [0.80300347, 0.79735813, 0.82185817, ..., 0.98173573, 0.98848795,\n",
       "        0.99070183],\n",
       "       [0.79735813, 0.82185817, 0.80543872, ..., 0.98848795, 0.99070183,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the Matrix\n",
    "\n",
    "We need to add a new matrix dimension to accommodate the indicator (predictor). \n",
    "\n",
    "NumPy matrices are tensors (3D) and essentially we need to specify that our matrix consists of **90 days** (dimension x) times **total days in data set** (dimension y) times **1 value per matrix cell (scalar)** (dimension z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.pixabay.com/photo/2015/03/22/17/34/cubic-684961_1280.jpg\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to add the stock value of somebody else together with the the past 90 days of MasterCard, we need to change the length of the 3 dimension to  2.  RNN training tables are 3D!!! Read: [Reshaping NumPy Array | Numpy Array Reshape Examples](https://backtobazics.com/python/python-reshaping-numpy-array-examples/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the data matrix, we retain the 2 original dimensions and add a third of depth=1\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN initialization\n",
    "\n",
    "- Import the sequential model from the Keras API;\n",
    "- Import the Dense layer template from the Keras API;\n",
    "- Import the LSTM model from the Keras API\n",
    "- Create an instance of the sequential model called regressor because we want to predict a continuous value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the RNN as a sequence of layers\n",
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add First Layer\n",
    "\n",
    "We first add an object of the LSTM class! \n",
    "\n",
    "- The first argument is the number of units or LSTM memory cells. Include many neurons to address the high dimensionality of the problem; say 50 neurons! \n",
    "- Second arg: return sequences = true; stacked LSTM !\n",
    "- Third arg: input 3D shape: observations vs time steps vs number of indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the LSTM layer\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape =  (X_train.shape[1], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the argument is the dropout rate to ignore in the layers (20%), \n",
    "# i.e. 50 units * 20% = 10 units will be dropped each time\n",
    "regressor.add(Dropout(0.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add More Layers\n",
    "\n",
    "We can add more LSTM layers but along with Dropout regularization to make sure we avoid overfitting! \n",
    "\n",
    "We don‚Äôt need to add the shape of the layer again because it is recognized automatically from the number of input units.\n",
    "\n",
    "The last layer does not return a sequence but connected directly to a fully connected output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "# we removed the return_sequences because we no longer return a \n",
    "# sequence but a value instead\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Output Layer & Compile\n",
    "\n",
    "The output has 1 dimension , i.e. one value to be predicted thus or output fully connected layer has dimensionality = 1.\n",
    "\n",
    "- **Optimizer**: rmsprop is recommended in the Keras documentation. The Adam optimizer is also a powerful choice.\n",
    "- **Loss function**: regression problems take the mean square error as most common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and deploy the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the RNN to the Training set\n",
    "\n",
    "We now want to train our RNN using the data in our **Training Set X** and **predictors in y** (ground truth in this case). Parameters that can be specified are the:\n",
    "\n",
    "- **Batch size**:  update the cell weights not on every stock price on every batch_size values; \n",
    "- **Number of epochs**: how many iterations to be used, i.e. number of forward and backward propagations for the update of the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "81/81 [==============================] - 18s 223ms/step - loss: 0.0078\n",
      "Epoch 2/100\n",
      "81/81 [==============================] - 18s 224ms/step - loss: 0.0017\n",
      "Epoch 3/100\n",
      "81/81 [==============================] - 17s 215ms/step - loss: 0.0015\n",
      "Epoch 4/100\n",
      "81/81 [==============================] - 17s 213ms/step - loss: 0.0013\n",
      "Epoch 5/100\n",
      "81/81 [==============================] - 18s 227ms/step - loss: 0.0013\n",
      "Epoch 6/100\n",
      "81/81 [==============================] - 18s 221ms/step - loss: 0.0013\n",
      "Epoch 7/100\n",
      "81/81 [==============================] - 18s 219ms/step - loss: 0.0013\n",
      "Epoch 8/100\n",
      "81/81 [==============================] - 19s 232ms/step - loss: 0.0011\n",
      "Epoch 9/100\n",
      "81/81 [==============================] - 18s 226ms/step - loss: 9.5853e-04\n",
      "Epoch 10/100\n",
      "81/81 [==============================] - 19s 231ms/step - loss: 0.0010\n",
      "Epoch 11/100\n",
      "81/81 [==============================] - 18s 226ms/step - loss: 0.0012\n",
      "Epoch 12/100\n",
      "81/81 [==============================] - 18s 223ms/step - loss: 9.5615e-04\n",
      "Epoch 13/100\n",
      "81/81 [==============================] - 18s 218ms/step - loss: 0.0010\n",
      "Epoch 14/100\n",
      "81/81 [==============================] - 19s 235ms/step - loss: 9.3163e-04\n",
      "Epoch 15/100\n",
      "81/81 [==============================] - 20s 249ms/step - loss: 7.9117e-04\n",
      "Epoch 16/100\n",
      "81/81 [==============================] - 21s 254ms/step - loss: 8.5629e-04\n",
      "Epoch 17/100\n",
      "81/81 [==============================] - 18s 225ms/step - loss: 8.9617e-04\n",
      "Epoch 18/100\n",
      "81/81 [==============================] - 18s 218ms/step - loss: 8.2639e-04\n",
      "Epoch 19/100\n",
      "81/81 [==============================] - 19s 236ms/step - loss: 7.4921e-04\n",
      "Epoch 20/100\n",
      "81/81 [==============================] - 20s 249ms/step - loss: 7.8363e-04\n",
      "Epoch 21/100\n",
      "81/81 [==============================] - 20s 241ms/step - loss: 9.3652e-04\n",
      "Epoch 22/100\n",
      "81/81 [==============================] - 19s 234ms/step - loss: 8.7237e-04\n",
      "Epoch 23/100\n",
      "81/81 [==============================] - 18s 224ms/step - loss: 8.8290e-04\n",
      "Epoch 24/100\n",
      "81/81 [==============================] - 19s 231ms/step - loss: 8.9868e-04\n",
      "Epoch 25/100\n",
      "81/81 [==============================] - 19s 235ms/step - loss: 7.8181e-04\n",
      "Epoch 26/100\n",
      "81/81 [==============================] - 19s 231ms/step - loss: 8.1412e-04\n",
      "Epoch 27/100\n",
      "81/81 [==============================] - 18s 228ms/step - loss: 7.7944e-04\n",
      "Epoch 28/100\n",
      "81/81 [==============================] - 19s 232ms/step - loss: 9.9252e-04\n",
      "Epoch 29/100\n",
      "81/81 [==============================] - 19s 237ms/step - loss: 8.2473e-04\n",
      "Epoch 30/100\n",
      "81/81 [==============================] - 19s 237ms/step - loss: 6.9817e-04\n",
      "Epoch 31/100\n",
      "81/81 [==============================] - 20s 248ms/step - loss: 6.9901e-04\n",
      "Epoch 32/100\n",
      "81/81 [==============================] - 20s 242ms/step - loss: 7.5735e-04\n",
      "Epoch 33/100\n",
      "81/81 [==============================] - 22s 270ms/step - loss: 6.0921e-04\n",
      "Epoch 34/100\n",
      "81/81 [==============================] - 19s 235ms/step - loss: 8.2301e-04\n",
      "Epoch 35/100\n",
      "81/81 [==============================] - 20s 252ms/step - loss: 6.4080e-04\n",
      "Epoch 36/100\n",
      "81/81 [==============================] - 19s 229ms/step - loss: 6.2999e-04\n",
      "Epoch 37/100\n",
      "81/81 [==============================] - 19s 230ms/step - loss: 6.3762e-04\n",
      "Epoch 38/100\n",
      "81/81 [==============================] - 18s 227ms/step - loss: 6.6296e-04\n",
      "Epoch 39/100\n",
      "81/81 [==============================] - 18s 226ms/step - loss: 6.1328e-04\n",
      "Epoch 40/100\n",
      "81/81 [==============================] - 18s 224ms/step - loss: 6.0698e-04\n",
      "Epoch 41/100\n",
      "81/81 [==============================] - 18s 221ms/step - loss: 6.4430e-04\n",
      "Epoch 42/100\n",
      "81/81 [==============================] - 18s 226ms/step - loss: 6.7155e-04\n",
      "Epoch 43/100\n",
      "81/81 [==============================] - 18s 222ms/step - loss: 6.2189e-04\n",
      "Epoch 44/100\n",
      "81/81 [==============================] - 20s 244ms/step - loss: 6.8977e-04\n",
      "Epoch 45/100\n",
      "81/81 [==============================] - 19s 231ms/step - loss: 7.0300e-04\n",
      "Epoch 46/100\n",
      "81/81 [==============================] - 19s 229ms/step - loss: 7.0502e-04\n",
      "Epoch 47/100\n",
      "81/81 [==============================] - 18s 227ms/step - loss: 8.3699e-04\n",
      "Epoch 48/100\n",
      "81/81 [==============================] - 18s 223ms/step - loss: 6.6404e-04\n",
      "Epoch 49/100\n",
      "81/81 [==============================] - 20s 246ms/step - loss: 6.1981e-04\n",
      "Epoch 50/100\n",
      "81/81 [==============================] - 19s 239ms/step - loss: 6.8952e-04\n",
      "Epoch 51/100\n",
      "81/81 [==============================] - 20s 248ms/step - loss: 5.8725e-04\n",
      "Epoch 52/100\n",
      "81/81 [==============================] - 19s 234ms/step - loss: 6.0910e-04\n",
      "Epoch 53/100\n",
      "81/81 [==============================] - 18s 218ms/step - loss: 6.5349e-04\n",
      "Epoch 54/100\n",
      "81/81 [==============================] - 18s 223ms/step - loss: 5.9527e-04\n",
      "Epoch 55/100\n",
      "81/81 [==============================] - 19s 233ms/step - loss: 6.2924e-04\n",
      "Epoch 56/100\n",
      "81/81 [==============================] - 19s 233ms/step - loss: 6.6357e-04\n",
      "Epoch 57/100\n",
      "81/81 [==============================] - 19s 240ms/step - loss: 6.1556e-04\n",
      "Epoch 58/100\n",
      "81/81 [==============================] - 19s 235ms/step - loss: 6.5588e-04\n",
      "Epoch 59/100\n",
      "81/81 [==============================] - 19s 234ms/step - loss: 7.8391e-04\n",
      "Epoch 60/100\n",
      "81/81 [==============================] - 19s 234ms/step - loss: 5.3933e-04\n",
      "Epoch 61/100\n",
      "81/81 [==============================] - 20s 245ms/step - loss: 6.1025e-04\n",
      "Epoch 62/100\n",
      "81/81 [==============================] - 20s 245ms/step - loss: 7.4475e-04\n",
      "Epoch 63/100\n",
      "81/81 [==============================] - 19s 231ms/step - loss: 6.1328e-04\n",
      "Epoch 64/100\n",
      "81/81 [==============================] - 19s 238ms/step - loss: 4.8114e-04\n",
      "Epoch 65/100\n",
      "81/81 [==============================] - 19s 237ms/step - loss: 5.5318e-04\n",
      "Epoch 66/100\n",
      "81/81 [==============================] - 19s 232ms/step - loss: 5.8555e-04\n",
      "Epoch 67/100\n",
      "81/81 [==============================] - 18s 227ms/step - loss: 5.9393e-04\n",
      "Epoch 68/100\n",
      "81/81 [==============================] - 21s 259ms/step - loss: 6.0370e-04\n",
      "Epoch 69/100\n",
      "81/81 [==============================] - 19s 233ms/step - loss: 5.6087e-04\n",
      "Epoch 70/100\n",
      "81/81 [==============================] - 18s 228ms/step - loss: 5.2548e-04\n",
      "Epoch 71/100\n",
      "81/81 [==============================] - 18s 222ms/step - loss: 5.5901e-04\n",
      "Epoch 72/100\n",
      "81/81 [==============================] - 18s 228ms/step - loss: 5.5302e-04\n",
      "Epoch 73/100\n",
      "81/81 [==============================] - 18s 225ms/step - loss: 5.9640e-04\n",
      "Epoch 74/100\n",
      "81/81 [==============================] - 18s 221ms/step - loss: 5.1869e-04\n",
      "Epoch 75/100\n",
      "81/81 [==============================] - 18s 221ms/step - loss: 6.1897e-04\n",
      "Epoch 76/100\n",
      "81/81 [==============================] - 18s 223ms/step - loss: 5.8773e-04\n",
      "Epoch 77/100\n",
      "81/81 [==============================] - 18s 228ms/step - loss: 7.2073e-04\n",
      "Epoch 78/100\n",
      "81/81 [==============================] - 19s 232ms/step - loss: 5.9310e-04\n",
      "Epoch 79/100\n",
      "81/81 [==============================] - 19s 234ms/step - loss: 6.3541e-04\n",
      "Epoch 80/100\n",
      "81/81 [==============================] - 18s 226ms/step - loss: 5.7147e-04\n",
      "Epoch 81/100\n",
      "81/81 [==============================] - 18s 227ms/step - loss: 6.2579e-04\n",
      "Epoch 82/100\n",
      "81/81 [==============================] - 19s 233ms/step - loss: 6.0245e-04\n",
      "Epoch 83/100\n",
      "81/81 [==============================] - 18s 226ms/step - loss: 5.5703e-04\n",
      "Epoch 84/100\n",
      "81/81 [==============================] - 18s 228ms/step - loss: 5.9746e-04\n",
      "Epoch 85/100\n",
      "81/81 [==============================] - 18s 227ms/step - loss: 5.5819e-04\n",
      "Epoch 86/100\n",
      "81/81 [==============================] - 19s 232ms/step - loss: 5.3977e-04\n",
      "Epoch 87/100\n",
      "81/81 [==============================] - 21s 262ms/step - loss: 5.3467e-04\n",
      "Epoch 88/100\n",
      "81/81 [==============================] - 21s 262ms/step - loss: 5.3746e-04\n",
      "Epoch 89/100\n",
      "81/81 [==============================] - 22s 278ms/step - loss: 5.6048e-04\n",
      "Epoch 90/100\n",
      "81/81 [==============================] - 21s 264ms/step - loss: 6.1913e-04\n",
      "Epoch 91/100\n",
      "81/81 [==============================] - 20s 249ms/step - loss: 5.5034e-04\n",
      "Epoch 92/100\n",
      "81/81 [==============================] - 22s 266ms/step - loss: 5.3532e-04\n",
      "Epoch 93/100\n",
      "81/81 [==============================] - 21s 257ms/step - loss: 5.5844e-04\n",
      "Epoch 94/100\n",
      "81/81 [==============================] - 21s 264ms/step - loss: 5.6254e-04\n",
      "Epoch 95/100\n",
      "81/81 [==============================] - 21s 261ms/step - loss: 5.5423e-04\n",
      "Epoch 96/100\n",
      "81/81 [==============================] - 20s 244ms/step - loss: 5.3678e-04\n",
      "Epoch 97/100\n",
      "81/81 [==============================] - 21s 262ms/step - loss: 5.8838e-04\n",
      "Epoch 98/100\n",
      "81/81 [==============================] - 21s 261ms/step - loss: 5.3549e-04\n",
      "Epoch 99/100\n",
      "81/81 [==============================] - 19s 230ms/step - loss: 5.1501e-04\n",
      "Epoch 100/100\n",
      "81/81 [==============================] - 19s 231ms/step - loss: 5.8773e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f65b40d5310>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the RNN to the Training set\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Predictions\n",
    "\n",
    "Create a data-frame by importing the MasterCard Stock Price Test set for January 2017 using pandas and make it a numpy array.\n",
    "\n",
    "There are 20 financial days in one month, weekends are excluded!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>273.929993</td>\n",
       "      <td>279.309998</td>\n",
       "      <td>271.640015</td>\n",
       "      <td>274.160004</td>\n",
       "      <td>272.359222</td>\n",
       "      <td>4988100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-08-02</td>\n",
       "      <td>272.000000</td>\n",
       "      <td>272.500000</td>\n",
       "      <td>265.679993</td>\n",
       "      <td>269.450012</td>\n",
       "      <td>267.680206</td>\n",
       "      <td>4438000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-08-05</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>262.649994</td>\n",
       "      <td>253.899994</td>\n",
       "      <td>256.839996</td>\n",
       "      <td>255.152969</td>\n",
       "      <td>6861300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-08-06</td>\n",
       "      <td>261.799988</td>\n",
       "      <td>265.149994</td>\n",
       "      <td>260.179993</td>\n",
       "      <td>264.679993</td>\n",
       "      <td>262.941498</td>\n",
       "      <td>4918500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-08-07</td>\n",
       "      <td>263.399994</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>260.250000</td>\n",
       "      <td>269.309998</td>\n",
       "      <td>267.541077</td>\n",
       "      <td>4476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-08-08</td>\n",
       "      <td>272.000000</td>\n",
       "      <td>278.079987</td>\n",
       "      <td>271.600006</td>\n",
       "      <td>278.040009</td>\n",
       "      <td>276.213745</td>\n",
       "      <td>4135500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-08-09</td>\n",
       "      <td>275.510010</td>\n",
       "      <td>276.899994</td>\n",
       "      <td>273.079987</td>\n",
       "      <td>274.950012</td>\n",
       "      <td>273.144073</td>\n",
       "      <td>3410500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-08-12</td>\n",
       "      <td>272.850006</td>\n",
       "      <td>274.980011</td>\n",
       "      <td>268.890015</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>268.226563</td>\n",
       "      <td>3322600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-08-13</td>\n",
       "      <td>269.829987</td>\n",
       "      <td>275.369995</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>274.529999</td>\n",
       "      <td>272.726807</td>\n",
       "      <td>3899800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-08-14</td>\n",
       "      <td>269.820007</td>\n",
       "      <td>272.470001</td>\n",
       "      <td>265.850006</td>\n",
       "      <td>267.149994</td>\n",
       "      <td>265.395264</td>\n",
       "      <td>3750300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-08-15</td>\n",
       "      <td>269.279999</td>\n",
       "      <td>273.149994</td>\n",
       "      <td>268.089996</td>\n",
       "      <td>271.890015</td>\n",
       "      <td>270.104156</td>\n",
       "      <td>3364300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-08-16</td>\n",
       "      <td>274.980011</td>\n",
       "      <td>275.959991</td>\n",
       "      <td>273.790009</td>\n",
       "      <td>274.359985</td>\n",
       "      <td>272.557861</td>\n",
       "      <td>2718100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-08-19</td>\n",
       "      <td>277.000000</td>\n",
       "      <td>278.570007</td>\n",
       "      <td>275.209991</td>\n",
       "      <td>278.070007</td>\n",
       "      <td>276.243530</td>\n",
       "      <td>2502300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-08-20</td>\n",
       "      <td>278.399994</td>\n",
       "      <td>279.559998</td>\n",
       "      <td>275.850006</td>\n",
       "      <td>277.040009</td>\n",
       "      <td>275.220306</td>\n",
       "      <td>2583600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-08-21</td>\n",
       "      <td>279.640015</td>\n",
       "      <td>282.100006</td>\n",
       "      <td>279.290009</td>\n",
       "      <td>282.010010</td>\n",
       "      <td>280.157654</td>\n",
       "      <td>2492200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-08-22</td>\n",
       "      <td>282.109985</td>\n",
       "      <td>282.890015</td>\n",
       "      <td>276.760010</td>\n",
       "      <td>280.769989</td>\n",
       "      <td>278.925812</td>\n",
       "      <td>2975800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-08-23</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>282.089996</td>\n",
       "      <td>270.209991</td>\n",
       "      <td>271.890015</td>\n",
       "      <td>270.104156</td>\n",
       "      <td>4415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>274.880005</td>\n",
       "      <td>276.429993</td>\n",
       "      <td>272.980011</td>\n",
       "      <td>276.429993</td>\n",
       "      <td>274.614288</td>\n",
       "      <td>2184400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>276.739990</td>\n",
       "      <td>279.089996</td>\n",
       "      <td>274.910004</td>\n",
       "      <td>276.640015</td>\n",
       "      <td>274.822937</td>\n",
       "      <td>2169400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-08-28</td>\n",
       "      <td>275.230011</td>\n",
       "      <td>278.649994</td>\n",
       "      <td>273.369995</td>\n",
       "      <td>278.239990</td>\n",
       "      <td>276.412415</td>\n",
       "      <td>2306900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>281.000000</td>\n",
       "      <td>281.980011</td>\n",
       "      <td>278.880005</td>\n",
       "      <td>281.380005</td>\n",
       "      <td>279.531799</td>\n",
       "      <td>2297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2019-08-30</td>\n",
       "      <td>282.589996</td>\n",
       "      <td>282.959991</td>\n",
       "      <td>278.730011</td>\n",
       "      <td>281.369995</td>\n",
       "      <td>279.521851</td>\n",
       "      <td>2600300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date        Open        High         Low       Close   Adj Close  \\\n",
       "0   2019-08-01  273.929993  279.309998  271.640015  274.160004  272.359222   \n",
       "1   2019-08-02  272.000000  272.500000  265.679993  269.450012  267.680206   \n",
       "2   2019-08-05  260.000000  262.649994  253.899994  256.839996  255.152969   \n",
       "3   2019-08-06  261.799988  265.149994  260.179993  264.679993  262.941498   \n",
       "4   2019-08-07  263.399994  270.000000  260.250000  269.309998  267.541077   \n",
       "5   2019-08-08  272.000000  278.079987  271.600006  278.040009  276.213745   \n",
       "6   2019-08-09  275.510010  276.899994  273.079987  274.950012  273.144073   \n",
       "7   2019-08-12  272.850006  274.980011  268.890015  270.000000  268.226563   \n",
       "8   2019-08-13  269.829987  275.369995  268.000000  274.529999  272.726807   \n",
       "9   2019-08-14  269.820007  272.470001  265.850006  267.149994  265.395264   \n",
       "10  2019-08-15  269.279999  273.149994  268.089996  271.890015  270.104156   \n",
       "11  2019-08-16  274.980011  275.959991  273.790009  274.359985  272.557861   \n",
       "12  2019-08-19  277.000000  278.570007  275.209991  278.070007  276.243530   \n",
       "13  2019-08-20  278.399994  279.559998  275.850006  277.040009  275.220306   \n",
       "14  2019-08-21  279.640015  282.100006  279.290009  282.010010  280.157654   \n",
       "15  2019-08-22  282.109985  282.890015  276.760010  280.769989  278.925812   \n",
       "16  2019-08-23  280.000000  282.089996  270.209991  271.890015  270.104156   \n",
       "17  2019-08-26  274.880005  276.429993  272.980011  276.429993  274.614288   \n",
       "18  2019-08-27  276.739990  279.089996  274.910004  276.640015  274.822937   \n",
       "19  2019-08-28  275.230011  278.649994  273.369995  278.239990  276.412415   \n",
       "20  2019-08-29  281.000000  281.980011  278.880005  281.380005  279.531799   \n",
       "21  2019-08-30  282.589996  282.959991  278.730011  281.369995  279.521851   \n",
       "\n",
       "     Volume  \n",
       "0   4988100  \n",
       "1   4438000  \n",
       "2   6861300  \n",
       "3   4918500  \n",
       "4   4476000  \n",
       "5   4135500  \n",
       "6   3410500  \n",
       "7   3322600  \n",
       "8   3899800  \n",
       "9   3750300  \n",
       "10  3364300  \n",
       "11  2718100  \n",
       "12  2502300  \n",
       "13  2583600  \n",
       "14  2492200  \n",
       "15  2975800  \n",
       "16  4415000  \n",
       "17  2184400  \n",
       "18  2169400  \n",
       "19  2306900  \n",
       "20  2297200  \n",
       "21  2600300  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the real stock price for January 1st 2009 - \n",
    "# December 31st 2019\n",
    "\n",
    "dataset_test = pd.read_csv('Mastercard_Test.csv')\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
    "real_stock_price.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[273.929993],\n",
       "       [272.      ],\n",
       "       [260.      ],\n",
       "       [261.799988],\n",
       "       [263.399994],\n",
       "       [272.      ],\n",
       "       [275.51001 ],\n",
       "       [272.850006],\n",
       "       [269.829987],\n",
       "       [269.820007],\n",
       "       [269.279999],\n",
       "       [274.980011],\n",
       "       [277.      ],\n",
       "       [278.399994],\n",
       "       [279.640015],\n",
       "       [282.109985],\n",
       "       [280.      ],\n",
       "       [274.880005],\n",
       "       [276.73999 ],\n",
       "       [275.230011],\n",
       "       [281.      ],\n",
       "       [282.589996]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_stock_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the stock price value for each day in January 2017, we need the values in the last 60 days.\n",
    "\n",
    "To obtain this **history** we need to combine both the training and test sets in one.\n",
    "\n",
    "If we were to use the training_set and test_set we would need to use the scaler  but that would change the actual test values.  Thus concatenate the original data frames!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2019\n",
    "\n",
    "# axis = 0 means concatenate the lines (i.e. vertical axis)\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2683"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_total.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the difference in the length of the first two gives us \n",
    "# the first day in 2019, and we need to go back 90 days to get the necessary range\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 90:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we did not use iloc from panda so lets reshape the numpy array for \n",
    "# compatibility: i.e. all the values from input lines to be stacked in one \n",
    "# column. The -1 means that the numpy has no knowledge of how the \n",
    "# values were stored in lines. The 1 means we want to them in one \n",
    "# column.\n",
    "\n",
    "inputs = inputs.reshape(-1,1) \n",
    "\n",
    "# apply the feature scaler\n",
    "inputs = sc.transform(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each price in Jan. 2017 we need the **immediate 60 values** before it. \n",
    "2. We have 21 prices in January;\n",
    "3. We need a numpy 3D array of 60 prices (columns) times 21 days (rows) times 1 dependent variable \n",
    "4. We don‚Äôt need y_test. That is what we are trying to compute!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2019\n",
    "X_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 90 from inputs are from training set;\n",
    "for i in range(90, 112): \n",
    "    X_test.append(inputs[i-90:i, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test) # not 3D structure yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 3D structure\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to inverse the scaling\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price) \n",
    "predicted_stock_price.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the results\n",
    "\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real MasterCard Stock Price')\n",
    "\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted MasterCard Stock Price')\n",
    "\n",
    "plt.title('MasterCard Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('MasterCard Stock Price')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue line shows the trend of the stock for the month of August 2019. \n",
    "\n",
    "Some observations:\n",
    "- The prediction lags behind the actual price curve because the model cannot react to fast non-linear changes. Spikes are examples of fast non-linear changes\n",
    "- Model reacts pretty well to smooth changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the RMSE\n",
    "\n",
    "If we need to compute the RMSE for our Stock Price Prediction problem, we use the real stock price and predicted stock price as shown.\n",
    "\n",
    "Then consider dividing this RMSE by the range of the MasterCard Stock Price values of January 2017 to get a relative error, as opposed to an absolute error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.698434846147163"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = math.sqrt( mean_squared_error( real_stock_price[0:24,:], predicted_stock_price))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new data need to be placed in the same order/format  as in the case of the training/test sets.\n",
    "\n",
    "1. Getting more training data: we trained our model on the past 5 years of the  Google Stock Price but it would be even better to train it on the past 10 years.\n",
    "\n",
    "2. Increasing the number of time steps: the model remembered the stock price from the 60 previous financial days to predict the stock price of the next day. That‚Äôs because we chose a number of 60 time steps (3 months). You could try to increase the number of time steps, by choosing for example 120 time steps (6 months).\n",
    "\n",
    "3. Adding some other indicators: if you have the financial instinct that the stock price of some other companies might be correlated to the one of Google, you could add this other stock price as a new indicator in the training data.\n",
    "\n",
    "4. Adding more LSTM layers: we built a RNN with four LSTM layers but you could try with even more.\n",
    "\n",
    "5. Adding more neurons in the LSTM layers: we highlighted the fact that we needed a high number of neurons in the LSTM layers to respond better to the complexity of the problem and we chose to include 50 neurons in each of our 4 LSTM layers. You could try an architecture with even more neurons in each of the 4 (or more) LSTM layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning the RNN\n",
    "\n",
    "Parameter Tuning on the RNN model: we are dealing with a Regression problem because we predict a continuous outcome.\n",
    "\n",
    "**Tip**: replace: scoring = 'accuracy' by scoring = 'neg_mean_squared_error' in the GridSearchCV class parameters as we did in the ANN case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
